# ğŸš€ LLaMA 3.1 / Mistral / TinyLlama Fine-Tuning Pipeline

This repository provides an **interactive fine-tuning pipeline** for **LLaMA 3.1**, **Mistral v0.3**, and **TinyLlama**, using **Hugging Face Transformers** and **PyTorch**. The pipeline allows users to select a model via a dropdown and fine-tune it on a dataset.

## ğŸ“Œ Features
- âœ… **Supports multiple models** (LLaMA 3.1, Mistral v0.3, TinyLlama).  
- âœ… **Interactive dropdown for model selection**.  
- âœ… **Automatic model loading & fine-tuning**.  
- âœ… **Memory-optimized training using 8-bit mode**.  
- âœ… **Resumes downloads to prevent interruptions**.  
- âœ… **Supports GPU acceleration with `torch.bfloat16`**.  

---

## ğŸ“¥ Installation
Ensure you have the necessary dependencies installed:
```bash
pip install transformers datasets torch ipywidgets
If using Hugging Face private models, authenticate:
```

```bash
huggingface-cli login
```
## âš¡ Supported Models
|Model| Hugging Face ID|
|LLaMA 3.1 (8B)|	meta-llama/Llama-3.1-8B-Instruct|
|Mistral v0.3 (7B)|	mistralai/Mistral-7B-Instruct-v0.3|
|TinyLlama 1.1B|	TinyLlama/TinyLlama-1.1B-chat-v1.0|

## ğŸš€ Training Optimization
This pipeline leverages memory-efficient techniques to optimize fine-tuning performance:

- âœ… torch.bfloat16 â†’ Uses mixed precision for speed.

- âœ… resume_download=True â†’ Continues interrupted downloads.

- âœ… device_map="auto" â†’ Automatically assigns model to the best available GPU.

## ğŸ“œ License
This project is licensed under the MIT License.

## ğŸ‘¨â€ğŸ’» Author
Developed by Team synÎ±pse. If you have any questions, feel free to reach out!

