{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cedf07f-1d9f-4712-b047-fdb22e2b6856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.4.1+cu121)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.5.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-2.2.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/conda/lib/python3.11/site-packages (from optuna) (1.13.3)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from optuna) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from optuna) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/conda/lib/python3.11/site-packages (from optuna) (2.0.35)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from optuna) (4.66.5)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.11/site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.77)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "Downloading transformers-4.50.3-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m161.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.5.2-py3-none-any.whl (345 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m152.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m161.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.2.1-py3-none-any.whl (277 kB)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading aiohttp-3.11.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m156.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m122.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m182.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
      "Downloading multidict-6.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (133 kB)\n",
      "Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=e9c15f8f56047b87eb38b33a8d469314876051342ad6f88152814aff8233401f\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: sentencepiece, xxhash, safetensors, regex, propcache, multidict, frozenlist, dill, colorlog, aiohappyeyeballs, absl-py, yarl, nltk, multiprocess, huggingface-hub, aiosignal, tokenizers, rouge_score, optuna, aiohttp, transformers, accelerate, datasets, evaluate\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.9\n",
      "    Uninstalling dill-0.3.9:\n",
      "      Successfully uninstalled dill-0.3.9\n",
      "Successfully installed absl-py-2.2.1 accelerate-1.5.2 aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 colorlog-6.9.0 datasets-3.5.0 dill-0.3.8 evaluate-0.4.3 frozenlist-1.5.0 huggingface-hub-0.29.3 multidict-6.2.0 multiprocess-0.70.16 nltk-3.9.1 optuna-4.2.1 propcache-0.3.1 regex-2024.11.6 rouge_score-0.1.2 safetensors-0.5.3 sentencepiece-0.2.0 tokenizers-0.21.1 transformers-4.50.3 xxhash-3.5.0 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna datasets transformers torch accelerate sentencepiece nltk absl-py rouge_score evaluate huggingface_hub onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272a72a1-310d-4d0d-8272-e417462f69ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8e7902695347e3973ba9af63c08e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()  # This will prompt you to enter your Hugging Face token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2075af8d-6697-4a8f-8373-f6da79a7368b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d0d21d9a044faf87a1e81a65a7298f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', options=('Llama 3', 'Mistral', 'SmolLM'), style=DescriptionStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5780cb50f54c0c87af2b5218c06189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Confirm Selection', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8076e9ec4e42ce8d6cf576b5052028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Define available models\n",
    "model_options = [\"Llama 3\", \"Mistral\", \"SmolLM\"]\n",
    "\n",
    "# Create a dropdown widget\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=model_options,\n",
    "    description=\"Model:\",\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Create a confirmation button\n",
    "confirm_button = widgets.Button(\n",
    "    description=\"Confirm Selection\",\n",
    "    button_style=\"success\"\n",
    ")\n",
    "\n",
    "# Output widget to display confirmation\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_confirm_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        selected_model = model_dropdown.value\n",
    "        print(f\"Selected Model: {selected_model}\")\n",
    "\n",
    "# Attach event handler\n",
    "confirm_button.on_click(on_confirm_clicked)\n",
    "\n",
    "# Display widgets\n",
    "display(model_dropdown, confirm_button, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b60e8f4-12de-48a4-8980-1daaefa8df6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SmolLM (TinyLlama/TinyLlama-1.1B-Chat-v1.0)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:862: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmolLM loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Function to load the selected model\n",
    "def load_model(model_name):\n",
    "    model_map = {\n",
    "        \"Llama 3\": \"meta-llama/Llama-3.1-8B\",\n",
    "        \"Mistral\": \"mistralai/Mistral-7B-v0.1\",\n",
    "        \"SmolLM\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    }\n",
    "\n",
    "    if model_name in model_map:\n",
    "        model_id = model_map[model_name]\n",
    "        print(f\"Loading {model_name} ({model_id})...\")\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", use_auth_token=True)\n",
    "\n",
    "        print(f\"{model_name} loaded successfully!\")\n",
    "        return model, tokenizer\n",
    "    else:\n",
    "        print(\"Invalid model selection.\")\n",
    "        return None, None\n",
    "\n",
    "# Load the model based on user selection\n",
    "selected_model = model_dropdown.value\n",
    "model, tokenizer = load_model(selected_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aea9d980-622b-44e0-9299-aad55a5aa3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44fe9b0037040dda16774225047a37a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='📂 File:', placeholder='Enter file path (e.g., /path/to/dataset.csv)')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429caf7b277f44339e9c8d592615ab8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Upload & Process', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d10548823648e2addf96d485bc710a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import ipywidgets as widgets\n",
    "import os , sys\n",
    "from IPython.display import display\n",
    "\n",
    "# Output widget for capturing logs\n",
    "output_widget = widgets.Output()\n",
    "\n",
    "# File path input widget\n",
    "file_path_input = widgets.Text(\n",
    "    placeholder=\"Enter file path (e.g., /path/to/dataset.csv)\",\n",
    "    description=\"📂 File:\"\n",
    ")\n",
    "upload_button = widgets.Button(description=\"Upload & Process\")\n",
    "display(file_path_input, upload_button, output_widget)  # Display widgets\n",
    "\n",
    "# Function to handle file processing\n",
    "def handle_file_upload(_):\n",
    "    with output_widget:\n",
    "        output_widget.clear_output()  # Clear previous output\n",
    "        \n",
    "        file_path = file_path_input.value.strip()\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"❌ File not found: {file_path}\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n✅ Uploaded file: {file_path}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Process dataset\n",
    "        process_dataset(file_path)\n",
    "\n",
    "# Bind button click to function\n",
    "upload_button.on_click(handle_file_upload)\n",
    "\n",
    "# Function to process dataset\n",
    "def process_dataset(file_path):\n",
    "    with output_widget:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Keep only relevant columns\n",
    "            required_columns = ['description', 'essay']\n",
    "            df = df[required_columns].dropna()\n",
    "            \n",
    "            print(\"\\n✅ Dataset successfully loaded and filtered\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            # ✅ Train-Validation Split (80% Train, 20% Validation)\n",
    "            df = df.sample(frac=1, random_state=42)  # Shuffle dataset\n",
    "            train_size = int(0.8 * len(df))\n",
    "            train_df, val_df = df[:train_size].copy(), df[train_size:].copy()\n",
    "\n",
    "            print(f\"\\n📊 Dataset split: {len(train_df)} train / {len(val_df)} validation\")\n",
    "            \n",
    "            # Preprocess dataset\n",
    "            train_df = preprocess_for_llm(train_df)\n",
    "            val_df = preprocess_for_llm(val_df)\n",
    "\n",
    "            # Save processed datasets\n",
    "            train_path = \"train_dataset.json\"\n",
    "            val_path = \"val_dataset.json\"\n",
    "            train_df.to_json(train_path, orient=\"records\", lines=True)\n",
    "            val_df.to_json(val_path, orient=\"records\", lines=True)\n",
    "\n",
    "            print(f\"\\n✅ Processed datasets saved: `{train_path}` (Train), `{val_path}` (Validation)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing file: {e}\")\n",
    "\n",
    "# Preprocessing (LLM-based)\n",
    "def preprocess_for_llm(df):\n",
    "    df['description'] = df['description'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    df['essay'] = df['essay'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    \n",
    "    max_length = 512\n",
    "    df['description'] = df['description'].apply(lambda x: x[:max_length] if isinstance(x, str) else x)\n",
    "    df['essay'] = df['essay'].apply(lambda x: x[:max_length] if isinstance(x, str) else x)\n",
    "    \n",
    "    print(\"\\n✨ Preprocessed Dataset Sample:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e815c3b-5dc5-4bb6-a875-cd925eab5d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import optuna\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f1368f3-6941-40ba-b984-017ac075df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggested default parameters based on the selected model\n",
    "model_recommendations = {\n",
    "    \"Llama 3\": {\"learning_rate\": 2e-5, \"batch_size\": 8, \"num_train_epochs\": 3, \"weight_decay\": 0.01},\n",
    "    \"Mistral\": {\"learning_rate\": 3e-5, \"batch_size\": 16, \"num_train_epochs\": 4, \"weight_decay\": 0.02},\n",
    "    \"SmolLM\": {\"learning_rate\": 5e-5, \"batch_size\": 8, \"num_train_epochs\": 3, \"weight_decay\": 0.01},\n",
    "}\n",
    "# Set selected model parameters (assuming 'selected_model' is defined earlier)\n",
    "selected_model_params = model_recommendations.get(selected_model, model_recommendations[\"Llama 3\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4cb232a-f334-4453-b206-41f3c5bf9137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fc6938aef84d8aaac4fe96af4a0d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "894f8fb7fcf448f391d5d8fb01d60992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define processed dataset paths\n",
    "train_path = \"train_dataset.json\"\n",
    "val_path = \"val_dataset.json\"\n",
    "\n",
    "# Ensure datasets are processed before loading\n",
    "if os.path.exists(train_path) and os.path.exists(val_path):\n",
    "    dataset = load_dataset(\"json\", data_files={\"train\": train_path, \"validation\": val_path})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bf6cc3b0-1965-4bfd-9bf1-a4c05fe763dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b7922961a0418a89e7c462a43f8ae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1788 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9f015c06524da8a15ccc1704458dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset features: ['description', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function\n",
    "tokenized_dataset = dataset.map(lambda x: tokenizer(x[\"essay\"], truncation=True, padding=\"max_length\", max_length=512), batched=True, remove_columns=[\"essay\"])\n",
    "\n",
    "print(\"Tokenized dataset features:\", tokenized_dataset[\"train\"].column_names)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6374af04-57c7-4408-994f-eb6cecaa213f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e4e350a01d486c9a342f17b1c83062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=5e-05, description='Learning Rate:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56327d92ec04759883ff95f95d40655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Batch Size:', index=2, options=(2, 4, 8, 16, 24, 32, 64), value=8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c55c97c9e5490ba96fdaf99e36df65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=3, description='Epochs:', max=50, min=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292134777cee49e6a252a548526ac0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatText(value=0.01, description='Weight Decay:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef3090a3dec4179bde6ee554ab691cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Confirm Selection', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def on_confirm_selection(button):\n",
    "    global final_params\n",
    "    final_params = {\n",
    "        \"learning_rate\": learning_rate_widget.value,\n",
    "        \"batch_size\": batch_size_widget.value,\n",
    "        \"num_train_epochs\": epochs_widget.value,\n",
    "        \"weight_decay\": weight_decay_widget.value,\n",
    "    }\n",
    "    print(f\"Confirmed Hyperparameters: {final_params}\")\n",
    "\n",
    "# User input widgets for parameter selection\n",
    "learning_rate_widget = widgets.FloatText(value=selected_model_params[\"learning_rate\"], description=\"Learning Rate:\")\n",
    "batch_size_widget = widgets.Dropdown(options=[2, 4, 8, 16, 24, 32, 64], value=selected_model_params[\"batch_size\"], description=\"Batch Size:\")\n",
    "epochs_widget = widgets.IntSlider(min=2, max=50, value=selected_model_params[\"num_train_epochs\"], description=\"Epochs:\")\n",
    "weight_decay_widget = widgets.FloatText(value=selected_model_params[\"weight_decay\"], description=\"Weight Decay:\")\n",
    "\n",
    "# Confirmation button\n",
    "confirm_button = widgets.Button(description=\"Confirm Selection\", button_style='success')\n",
    "confirm_button.on_click(on_confirm_selection)\n",
    "\n",
    "display(learning_rate_widget, batch_size_widget, epochs_widget, weight_decay_widget, confirm_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a54b8ad-b583-470c-af90-fe8237bcc4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ User customized hyperparameters. Skipping Optuna optimization.\n",
      "📌 Customized Hyperparameters: {'learning_rate': 5e-05, 'batch_size': 2, 'num_train_epochs': 2, 'weight_decay': 0.01}\n",
      "Started Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='446' max='446' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [446/446 01:16, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.031400</td>\n",
       "      <td>3.516302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "# Function to check if user-selected parameters match recommended ones\n",
    "def user_selected_defaults():\n",
    "    return (\n",
    "        final_params[\"learning_rate\"] == selected_model_params[\"learning_rate\"] and\n",
    "        final_params[\"batch_size\"] == selected_model_params[\"batch_size\"] and\n",
    "        final_params[\"num_train_epochs\"] == selected_model_params[\"num_train_epochs\"] and\n",
    "        final_params[\"weight_decay\"] == selected_model_params[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "# Function to optimize hyperparameters using Optuna\n",
    "def objective(trial):\n",
    "    \"\"\"Hyperparameter tuning objective function using real model validation loss.\"\"\"\n",
    "    \n",
    "    # Suggest hyperparameters within reasonable ranges\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 5e-4, log=True)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 4, 32, step=4)\n",
    "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 3, 10)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "\n",
    "    # Update training arguments dynamically\n",
    "    training_args.learning_rate = learning_rate\n",
    "    training_args.per_device_train_batch_size = batch_size\n",
    "    training_args.num_train_epochs = num_train_epochs\n",
    "    training_args.weight_decay = weight_decay\n",
    "\n",
    "    # Create a new trainer instance with updated hyperparameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset.get(\"validation\", None),  # Ensure validation dataset is passed\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # Train for a single epoch and evaluate\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    eval_loss = eval_results[\"eval_loss\"]  # Get real validation loss\n",
    "    print(f\"Trial {trial.number}: Eval Loss = {eval_loss}, Params = {trial.params}\")\n",
    "\n",
    "    return eval_loss  # Minimize real validation loss\n",
    "\n",
    "\n",
    "# Run Optuna only if user-selected parameters match the recommended ones\n",
    "if user_selected_defaults():\n",
    "    print(\"✅ User selected default hyperparameters. Running Optuna optimization...\")\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=10)  # Run Optuna with real model evaluation\n",
    "        \n",
    "        # Get best hyperparameters\n",
    "    best_params = study.best_params\n",
    "    final_params = best_params\n",
    "        \n",
    "    print(\"Best Hyperparameters Found:\", final_params)\n",
    "else:\n",
    "    print(\"⚠ User customized hyperparameters. Skipping Optuna optimization.\")\n",
    "    print(\"📌 Customized Hyperparameters:\", final_params )\n",
    "    print(\"Started Training\")\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./fine_tuned_model\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=final_params[\"learning_rate\"],\n",
    "        per_device_train_batch_size=final_params[\"batch_size\"],\n",
    "        num_train_epochs=final_params[\"num_train_epochs\"],\n",
    "        weight_decay=final_params[\"weight_decay\"],\n",
    "        save_total_limit=1,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=100,\n",
    "        fp16=True,  # Disable fp16 if unstable  # Mixed precision training if GPU is available\n",
    "        report_to=\"none\",\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=4  \n",
    "    )\n",
    "        # Define the trainer\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset.get(\"validation\", None),  # Ensure validation dataset is passed\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a220ce5a-10bd-42db-b978-627422fb42d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.5163\n",
      "Perplexity (PPL): 33.6597\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from evaluate import load\n",
    "\n",
    "\n",
    "def compute_perplexity(eval_loss):\n",
    "    return math.exp(eval_loss)\n",
    "\n",
    "# Get validation loss from the Trainer\n",
    "eval_results = trainer.evaluate()\n",
    "eval_loss = eval_results[\"eval_loss\"]\n",
    "perplexity = compute_perplexity(eval_loss)\n",
    "\n",
    "print(f\"Validation Loss: {eval_loss:.4f}\")\n",
    "print(f\"Perplexity (PPL): {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f6e35d6f-33d4-4405-a215-ab3e578b2c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.0032\n",
      "ROUGE Score: {'rouge1': np.float64(0.09195957306972727), 'rouge2': np.float64(0.010081981236581005), 'rougeL': np.float64(0.06903890994649176), 'rougeLsum': np.float64(0.06906421498886611)}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from evaluate import load\n",
    "\n",
    "# Load evaluation metrics\n",
    "bleu = load(\"bleu\")\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Load validation dataset\n",
    "val_data = []\n",
    "with open(\"val_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        val_data.append(json.loads(line.strip()))\n",
    "\n",
    "# Extract inputs and references\n",
    "sample_inputs = [item[\"essay\"] for item in val_data]\n",
    "references = [[item[\"description\"]] for item in val_data]  # Ensure correct BLEU format\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(input_text):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")  # Move input to GPU\n",
    "    model.to(\"cuda\")  # Move model to GPU\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=50)  # Set max_new_tokens instead\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = [generate_text(text) for text in sample_inputs]\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Compute ROUGE score\n",
    "rouge_score = rouge.compute(predictions=predictions, references=[ref[0] for ref in references])\n",
    "\n",
    "# Print scores\n",
    "print(f\"BLEU Score: {bleu_score['bleu']:.4f}\")\n",
    "print(f\"ROUGE Score: {rouge_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0a7e6e1a-3299-458b-8bfa-bd4ed74776a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-Level Accuracy: 1.19%\n"
     ]
    }
   ],
   "source": [
    "def calculate_token_accuracy(predictions, references, tokenizer):\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = tokenizer.tokenize(pred)  # Tokenize prediction\n",
    "        ref_tokens = tokenizer.tokenize(ref[0])  # Tokenize reference (flatten list)\n",
    "\n",
    "        # Compute number of matching tokens\n",
    "        matches = sum(1 for p, r in zip(pred_tokens, ref_tokens) if p == r)\n",
    "\n",
    "        correct_tokens += matches\n",
    "        total_tokens += len(ref_tokens)\n",
    "\n",
    "    # Avoid division by zero\n",
    "    accuracy = (correct_tokens / total_tokens) * 100 if total_tokens > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "# Compute Token-Level Accuracy\n",
    "token_accuracy = calculate_token_accuracy(predictions, references, tokenizer)\n",
    "\n",
    "# Print the result\n",
    "print(f\"Token-Level Accuracy: {token_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8556a024-23a8-4a51-a98f-0be8c971f38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SmolLM (TinyLlama/TinyLlama-1.1B-Chat-v1.0)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:862: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmolLM loaded successfully!\n",
      "BLEU Score: 0.0035\n",
      "ROUGE Score: {'rouge1': np.float64(0.09475974087888797), 'rouge2': np.float64(0.010959319707140786), 'rougeL': np.float64(0.07189118712644443), 'rougeLsum': np.float64(0.07405909765944121)}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from evaluate import load\n",
    "\n",
    "# Load the original pre-trained model (before fine-tuning)\n",
    "original_model,original_tokenizer  = load_model(selected_model)\n",
    "\n",
    "# Load evaluation metrics\n",
    "bleu = load(\"bleu\")\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Load validation dataset\n",
    "val_data = []\n",
    "with open(\"val_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        val_data.append(json.loads(line.strip()))\n",
    "\n",
    "# Extract inputs and references\n",
    "sample_inputs = [item[\"essay\"] for item in val_data]\n",
    "references = [[item[\"description\"]] for item in val_data]  # Ensure correct BLEU format\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(input_text):\n",
    "    inputs = original_tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")  # Move input to GPU\n",
    "    original_model.to(\"cuda\")  # Move model to GPU\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = original_model.generate(**inputs, max_new_tokens=50)  # Set max_new_tokens instead\n",
    "    return original_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = [generate_text(text) for text in sample_inputs]\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Compute ROUGE score\n",
    "rouge_score = rouge.compute(predictions=predictions, references=[ref[0] for ref in references])\n",
    "\n",
    "# Print scores\n",
    "print(f\"BLEU Score: {bleu_score['bleu']:.4f}\")\n",
    "print(f\"ROUGE Score: {rouge_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "98736131-8f41-4d78-baa2-9fbb888fdae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0437171a-174e-44fa-94fe-dff7686025e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8efa87ee484b6b996b557e516aa17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Password(description='HF Token:', placeholder='Enter your Hugging Face token')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcd1669b7d147339069f268f29ad922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Repo Name:', placeholder='Enter repository name')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7019a2b0f6e243d3913d9be7344df2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Confirm', style=ButtonStyle(), tooltip='Click to create repository…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324ab23efece4446b9a96839f367dafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from huggingface_hub import create_repo\n",
    "\n",
    "def create_hf_repo(button):\n",
    "    hf_token = token_input.value\n",
    "    repo_name = repo_input.value\n",
    "    \n",
    "    if not hf_token or not repo_name:\n",
    "        output.clear_output()\n",
    "        with output:\n",
    "            print(\"Please enter both HF Token and Repo Name.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        create_repo(repo_name, token=hf_token, private=False, exist_ok=True, repo_type=\"model\")\n",
    "        output.clear_output()\n",
    "        with output:\n",
    "            print(f\"Repository '{repo_name}' created successfully!\")\n",
    "    except Exception as e:\n",
    "        output.clear_output()\n",
    "        with output:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "token_input = widgets.Password(\n",
    "    description='HF Token:',\n",
    "    placeholder='Enter your Hugging Face token'\n",
    ")\n",
    "\n",
    "repo_input = widgets.Text(\n",
    "    description='Repo Name:',\n",
    "    placeholder='Enter repository name'\n",
    ")\n",
    "\n",
    "confirm_button = widgets.Button(\n",
    "    description=\"Confirm\",\n",
    "    button_style='success',\n",
    "    tooltip=\"Click to create repository\"\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "confirm_button.on_click(create_hf_repo)\n",
    "\n",
    "display(token_input, repo_input, confirm_button, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a04c062f-42ba-441c-98f0-5249b16a54c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa12812df0b4409992bef3b255a52f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469c52d3ce5c4796bb2751dd5475aa8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/tomoe007/test/commit/c4203675e3d23e4079e2269e924c9d26fab1c9f4', commit_message='Upload tokenizer', commit_description='', oid='c4203675e3d23e4079e2269e924c9d26fab1c9f4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/tomoe007/test', endpoint='https://huggingface.co', repo_type='model', repo_id='tomoe007/test'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(repo_input.value, token=HF_TOKEN)\n",
    "tokenizer.push_to_hub(repo_input.value, token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "efb6a0fe-f77c-4b1f-b77a-7d6764863250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? i’m a student at the university of california, loma california.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = repo_input.value\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = tokenizer\n",
    "model = model\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Hello, how are you?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode and print\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b4dba0-c80e-4357-85c5-2c1d1ed90aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
